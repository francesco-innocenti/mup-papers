# ðŸ“‘ Î¼P Papers

![status](https://img.shields.io/badge/status-active-green)

A curated repository of papers on the maximal update parameterisation (Î¼P) and related ideas.

> Î¼P is an influential, theoretically grounded prescription for how to scale various neural network architectures such that the layer activations (and other quantities such as the learning rate) remain stable during training (neither shrink nor explode) with the model size (i.e. width and depth).


# Key original papers (width-only Î¼P)
* [Tensor Programs IV: Feature Learning in Infinite-Width Neural Networks](https://arxiv.org/abs/2011.14522)
* [Tensor Programs V: Tuning Large Neural Networks via Zero-Shot Hyperparameter Transfer](https://arxiv.org/abs/2203.03466)
* [Tensor Programs IVb: Adaptive Optimization in the Infinite-Width Limit](https://arxiv.org/abs/2308.01814)
* [A Spectral Condition for Feature Learning](https://arxiv.org/abs/2310.17813)


# Depth-Î¼P
* [Feature Learning in Infinite-Width Neural Networks](https://arxiv.org/abs/2011.14522)
* [Depthwise Hyperparameter Transfer in Residual Networks: Dynamics and Scaling Limit](https://arxiv.org/abs/2309.16620)
* [Super Consistency of Neural Network Landscapes and Learning Rate Transfer](https://proceedings.neurips.cc/paper_files/paper/2024/hash/ba1d33849b963efc6b5d3082ad68f480-Abstract-Conference.html)
